{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, math\n",
    "from json import loads, dumps\n",
    "from tqdm import tqdm\n",
    "\n",
    "e = math.exp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2173,
   "metadata": {},
   "outputs": [],
   "source": [
    "proprietes = {}\n",
    "data_set = pd.DataFrame()\n",
    "ansers = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proprietes():\n",
    "    global proprietes\n",
    "    with open(\"Network_Proprietes.json\") as file:\n",
    "        proprietes = file.read()\n",
    "    proprietes = loads(proprietes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    global ansers\n",
    "    global data_set\n",
    "    data_set = pd.read_csv(proprietes[\"Data Set\"] + \".csv\")\n",
    "    data_set = data_set.head(30)\n",
    "    data_set.drop(columns= proprietes[\"Drop colunms\"], inplace= True)\n",
    "    ansers = data_set[proprietes[\"Output\"]].copy()\n",
    "    ansers = np.ceil(ansers.clip(0, 1))\n",
    "    data_set.drop(columns= proprietes[\"Output\"], inplace= True)\n",
    "    data_set.reset_index(inplace= True, drop= True)\n",
    "    data_set = data_set.transpose()\n",
    "    data_set.reset_index(inplace= True, drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_neurons, hidden= True):\n",
    "        self.weights = pd.DataFrame(np.random.randn(num_inputs, num_neurons))\n",
    "        self.biases = pd.DataFrame(np.zeros((1, num_neurons)))\n",
    "        self.hidden = hidden\n",
    "\n",
    "    def forward(self, input, ansers= [], learn= False):\n",
    "        self.output = (self.weights.transpose()).dot(input.transpose())\n",
    "        self.output = self.output.transpose() + (self.biases.values.tolist()[0])\n",
    "        if self.hidden:\n",
    "            self.output.clip(0, inplace= True)\n",
    "        else:\n",
    "            self.output = (e ** ((self.output.transpose() - self.output.max(axis= 1)).transpose()))\n",
    "            self.output = (self.output.transpose() / self.output.sum(axis= 1)).transpose()\n",
    "            if learn:\n",
    "                avg = self.output[pd.get_dummies(ansers.transpose())].transpose().max().mean()\n",
    "                self.output = 1 - avg\n",
    "\n",
    "    def update(self):\n",
    "        self.weights += pd.DataFrame(np.random.randn(self.weights.shape[0], self.weights.shape[1]) * proprietes[\"Intensity\"])\n",
    "        self.biases += pd.DataFrame(np.random.randn(self.biases.shape[0], self.biases.shape[1]) * proprietes[\"Intensity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2177,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "def init_layers():\n",
    "    inputs = proprietes[\"Input\"]\n",
    "    for neuron_count, idx in zip(proprietes[\"Layers\"], range(len(proprietes[\"Layers\"]))):\n",
    "        if not idx+1 == len(proprietes[\"Layers\"]):\n",
    "            layers.append(Layer(inputs, neuron_count))\n",
    "            inputs = neuron_count\n",
    "        else:\n",
    "            layers.append(Layer(inputs, neuron_count, hidden= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2178,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_save = []\n",
    "\n",
    "def save_layers():\n",
    "    global layers_save\n",
    "    layers_save = layers.copy()\n",
    "\n",
    "def load_layers():\n",
    "    global layers\n",
    "    layers = layers_save.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(batch_num, batch_count, lowest_loss):\n",
    "    X = data_set.transpose().loc[(data_set.transpose().index % batch_count) == batch_num].copy()\n",
    "    y = ansers.loc[(ansers.index % batch_count) == batch_num].copy()\n",
    "    X.reset_index(inplace= True, drop= True)\n",
    "    y.reset_index(inplace= True, drop= True)\n",
    "    output = []\n",
    "    for layer in layers:\n",
    "        idx = layers.index(layer)\n",
    "        if idx == 0:\n",
    "            layer.forward(X)\n",
    "            output = layer.output\n",
    "        elif not idx+1 == len(layers):\n",
    "            layer.forward(output)\n",
    "            output = layer.output\n",
    "        else:\n",
    "            layer.forward(output, ansers= y, learn= True)\n",
    "            if layer.output > lowest_loss:\n",
    "                layer.update()\n",
    "                return None\n",
    "            else:\n",
    "                return layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    generations = proprietes[\"Generations\"]\n",
    "    batch_count = math.floor(data_set.shape[1] / proprietes[\"Batch Size\"])\n",
    "    lowest_loss = 1\n",
    "    pbar = tqdm(desc=\"Processing dataset\", colour='#0997FF', total= generations * batch_count)\n",
    "    for generation in range(generations):\n",
    "        for batch in range(batch_count):\n",
    "            pbar.desc = f\"Processing dataset (Loss: {round(lowest_loss, 4)}, Gen: {generation+1}/{generations})\"\n",
    "            pbar.update(1)\n",
    "            save_layers()\n",
    "            loss = run_batch(batch, batch_count, lowest_loss)\n",
    "            if not loss is None:\n",
    "                lowest_loss = loss\n",
    "                load_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset (Loss: 0.5, Gen: 100/100): 100%|\u001b[38;2;9;151;255m██████████\u001b[0m| 700/700 [00:07<00:00, 91.37it/s]\n"
     ]
    }
   ],
   "source": [
    "get_proprietes()\n",
    "get_dataset()\n",
    "init_layers()\n",
    "learn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
